## Spider

Spider là lớp định nghĩa cách cào một hay nhiều trang, bao gồm cách thực hiện thu thập thông tin và trích xuất dữ liệu có cấu trúc. Nói cách khác, Spider là nơi xác định hành vi tùy chỉnh để thu thập dữ liệu và phân tích cú pháp các trang cho một trang web cụ thể.

Chu kỳ cào dữ liệu đối với Spider như sau:
1. Bắt đầu bằng cách tạo ra request (yêu cầu) ban đầu để thu thập thông tin các URL đầu tiên và chỉ định hàm callback để được gọi với các phản hồi đã tải về từ các requests. Các requests đầu tiên được thực hiện bằng cách gọi phương thức `start_request()`, phương thức mặc định này tạo `Request` cho các URL được chỉ định trong phương thức `start_urls` và `parse` như hàm callback.
2. Trong hàm callback, phân tích phản hồi và trả về các đối tượng `Item`, `Request`. Những requests này cũng sẽ bao gồm một callback (có thể giống nhau) và sau đó sẽ được tải xuống bởi scrapy và sau đó phản hồi sẽ được  xử lý bởi các callback được chỉ định.
3. Trong các hàm callback, phân tích nội dung trang, thường dử dụng [Selectors](https://doc.scrapy.org/en/0.16/topics/selectors.html#topics-selectors "Selectors") (cũng có thể dùng BeautifulSoup, lxml, v.v..) và tạo ra các items có dữ liệu đã được phân tích.
4. Cuối cùng, các items được trả về từ spider sẽ được duy trì trong cơ sở dữ liệu (trong một số item pipeline) hoặc được ghi vào một tập tin sử dụng [Feed exports](https://doc.scrapy.org/en/0.16/topics/feed-exports.html#topics-feed-exports).

Mặc dù chu kỳ này được áp dụng cho bất kỳ loại spider nào nhưng có nhiều loại spider mặc định khác nhau trong Scrapy cho các mục đích khác nhau:

### crapy.Spider

`class scrapy.spider.Spider`

Đây là spider đơn giản nhất và tất cả các spider khác đều phải kế thừa. Nó không cung cấp bất kì một chức năng đặc biệt nào, chỉ cung cấp `start_menu()` gửi các request từ thuộc tính `start_urls` spider và gọi các phương thức `parse` của spider cho mỗi kết quả phản hồi.

### Spider arguments

Spider có thể nhận được các đối số (argument) sửa đổi hành vi của chúng. Một số cách sử dụng phổ biến cho đối số spider là xác định URL bắt đầu hoặc để hạn chế thu thập thông tin đến các phần nhất định của trang web nhưng chúng có thể được dùng để cấu hình bất kì chức năng nào của spider.

Các đối số spider có thể được truyền thông qua lệnh `crawl` dùng tùy chọn `-a`. Ví dụ:

`scrapy crawl myspider -a category=electronics`

Spider có thể truy cập các đối số trong phương thức *\__init__*:
```

 import scrapy
 class MySpider(scrapy.Spider):
    name = 'myspider'
    def __init__(self, category=None, *args, **kwargs):
        super(MySpider, self).__init__(*args, **kwargs)
        self.start_urls = ['http://www.example.com/categories/%s' % category]
        # ...
```
Phương thức mặc định *\__init__* sẽ lấy bất kỳ đối số spider nào và sao chép chúng vào spider như các thuộc tính. Ví dụ trên cũng có thể được viết như sau:
```

 import scrapy
 class MySpider(scrapy.Spider):
    name = 'myspider'
    def start_requests(self):
        yield scrapy.Request('http://www.example.com/categories/%s' % self.category)
```

### Các spider phổ biến

Scrapy đi kèm với một số spdier phổ biến hữu ích, có thể dùng để phân lớp spider. Mục tiêu của chúng là cung cấp chức năng tiện lợi cho một số trường hợp cào thông thường như theo tất cả các liên kết trên một trang web dựa trên các quy tắc nhất định, thu thập thông tin từ [Sitemaps](https://www.sitemaps.org/index.html) hoặc phân tích nguồn cấp dữ liệu XML/CSV.

#### CrawlSpider

`class scrapy.spiders.CrawlSpider`

Đây là spider phổ biến nhất được dùng để thu thập dữ liệu các trang web thông thường. Vì nó cung cấp cơ chế thuận tiện cho việc liên kết sau bằng cách định nghĩa một bộ quy tắc. Nó có thể không phù hợp nhất cho các trang web hoặc dự án cụ thể của người dùng, vì thế người dùng có thể bắt đầu từ CrawlSpider và ghi đè theo theo nhu cầu cho nhiều chức năng tùy chỉnh hoặc chỉ thực hiện spider riêng của mình.

#### XMLFeedSpider

`class scrapy.spiders.XMLFeedSpider`

XMLFeedSpider được thiết kế để phân tích các nguồn cung cấp dữ liệu XML bằng cách lặp lại chúng thông qua tên nút nhất định. Trình lặp có thể được chọn từ *iternodes*, *xml* và *html*.

#### CSVFeedSpider

`class scrapy.spiders.CSVFeedSpider`

Giống với XMLFeedSpider, CSVFeedSpider lặp qua các hàng thay vì các nút như XMLFeedSpider. Phương thức được gọi trong mỗi lần lặp là `parse_row()`.

#### SitemapSpider

`class scrapy.spiders.SitemapSpider`

SitemapSpider cho phép thu thập thông tin trang web bằng cách tìm kiếm các URL sử dụng [Sitemaps](https://www.sitemaps.org/index.html).
Spider này hỗ trợ sơ đồ trang web lồng nhau và tìm kiếm sơ đồ trang web URL từ [robots.txt](http://www.robotstxt.org/).

## Trích xuất (Extractor)và Spider
### Duyệt tất cả các trang
Cần xác định rõ cấp độ của trang:
-	Start_urls (cấp 1)
-	Trong Parse dẫn link đến trang cấp 2 (đây là trang cần duyệt hết các trang con)
-	Parse_next_page: thu thập dữ liệu
```lightning
def parse_articles_follow_next_page(self, response):
    for article in response.xpath("//article"):
        item = ArticleItem()

        ... trích xuất dữ liệu bài báo ở đây

        yield item

    next_page = response.css("ul.navigation > li.next-page > a::attr('href')")
    if next_page:
        url = response.urljoin(next_page[0].extract())
        yield scrapy.Request(url, self.parse_articles_follow_next_page)
```
**Lưu ý**: để kiểm tra kết quả trả về khi dùng các selector, sử dụng crapy shell
```lightning
Cú pháp: scrapy shell [url]
```
### Thêm thông tin vào hàm callback 
- Tình huống: khi duyệt các catelogy ->… -> trang web công ty -> lấy dữ liệu về công ty
- Vấn đề: muốn lưu thông tin catelogy của công ty
- Giải pháp: truyền thêm thông tin vào hàm callback.
```lightning
def parse_page1(self, response):
    item = MyItem()
    item['main_url'] = response.url
    request = scrapy.Request("http://www.example.com/some_page.html",
                             callback=self.parse_page2)
    request.meta['item'] = item
    yield request

def parse_page2(self, response):
    item = response.meta['item']
    item['other_url'] = response.url
    yield item
```
    
Xem bài thực hành phía dưới.
```lightning
Mẫu XPath :
vietnamese_title            //p[contains(string(),'Vietnamese Title:')]
english_title               //p[contains(string(),'English Title:')]
address                     //p[contains(string(), 'Address:') or contains(string(), 'Địa chỉ:') or contains(string(), 'Trụ sở chính:')]
province                    //p[contains(string(),'Province:')]
director                    //p[contains(string(),'Director:')]
tel                         //p[contains(string(),'Tel:') or contains(string(),'Điện thoại:')]
fax                         //p[contains(string(),'Fax:')]
email                       //p[contains(string(),'Email:')]
main_business               //p[contains(string(),'Main Business:')]
business                    //p[(contains(string(),'Business:') and not(contains(string(),'Main Business'))) 
                                or contains(string(),'Ngành nghề kinh doanh:')]
website                     //p[contains(string(),'Website:')]
company_title               //p[contains(string(),'Vietnamese Title:')]
```

fn:tokenize(//p[contains(string(),'Vietnamese Title:')], ':')[0] (todo: Tokenize là gì?)
#### Thủ thuật xpath để thu thập dữ liệu một trang web
- Tránh sử dụng ``contains(.//text(), ‘search text’) ``, thay vào đó nên dùng ``contains(., ‘search text’) ``
```lightning
Nên dùng: 
>>> xp("//a[contains(., 'Next Page')]")
kết quả: [u'<a href="#">Click here to go to the <strong>Next Page</strong></a>']
```
```lightning
Không nên dùng:
>>> xp("//a[contains(.//text(), 'Next Page')]")
kết quả: []
```
#### Cảnh giác sự khác nhau giữa //node[1] và (//node)[1]
- //node[1]: Chọn tất cả các nốt xuất hiện đầu tiên trong các nốt cùng nốt cha tương ứng
- (//node)[1]: Phát hiện tất cả các nốt xuất hiện trong trang web nhưng chỉ lấy một node đầu tiên
#### Khi xác định phần tử bằng class phải cụ thể và cần thiết
Xác định phần tử bởi một lớp Css:
```lightning
>>> sel.css(".content").extract()
[u'<p class="content text-wrap">Some content</p>']
>>> sel.css('.content').xpath('@class').extract()
[u'content text-wrap']
```
#### Mẹo nhỏ để lấy nội dung văn bản
```lightning
//*[not(self::script or self::style)]/text()[normalize-space(.)]
```
xpath này loại trừ nội dung từ script và các thẻ style và đồng thời bỏ qua các nốt văn bản chỉ có khoảng trắng
(nguồn:http://stackoverflow.com/a/19350897/2572383)