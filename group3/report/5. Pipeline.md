# Pipeline

Một item sau khi đã được cào bởi spider sẽ được chuyển đến Item Pipeline để xử lí thông qua một số thành phần được thực hiện tuần tự.

Mỗi thành phần item pipeline là một lớp Python thực hiện một phương thức đơn giản. Các lớp này nhận item và thực hiện các hành động đối với item và đồng thời quyết định xem item đó có tiếp thục pipeline hay bị bỏ hay không còn được xử lý.

Các ứng dụng tiêu biểu của item pipeline:
* Dọn dẹp dữ liệu HTML
* Xác nhận dữ liệu được cào (kiểm tra các items chứa các trường nhất định)
* Kiểm tra (và bỏ) các bản sao
* Lưu trữ các item được cào trong cơ sở dữ liệu

## Viết item pipeline
Mỗi thành phần item pipeline là một lớp Python thực hiện các phương thức:

`process_item(item, spider)`
  
  Phương thức này được gọi cho mỗi thành phần item pipeline và phải trả về một đối tượng `Item` (hoặc bất kỳ lớp con nào) hoặc đưa ra một ngoại lệ `DropItem`. Các item bị loại bỏ không còn được xử lý bởi pipeline nữa.
  
  **Các tham số:**
  * **item** (`item`) - item được cào          
  * **spider** (`BaseSpider`) - spider cào item

`open_spider(spider)`

  Phương thức này được gọi khi spider được mở
  
  **Các tham số:**
  **spider** (`BaseSpider`) - spider đã được mở

`close_spider(spider)`
  
  Phương thức này được gọi khi spider bị đóng
  
  **Các tham số:**
  **spider** (`BaseSpider`) - spider đã bị đóng

## Ví dụ về item pipeline
 ### Xác nhận giá cả và bỏ các mặt hàng không có giá
 Giả sử điều chỉnh thuộc tính `price` các mặt hàng không bao gồm thuế VAT (thuộc tính `price_excludes_vat`), và bỏ các items không có giá:
 ```
 from scrapy.exceptions import DropItem

 class PricePipeline(object):

    vat_factor = 1.15

    def process_item(self, item, spider):
        if item['price']:
            if item['price_excludes_vat']:
                item['price'] = item['price'] * self.vat_factor
            return item
        else:
            raise DropItem("Missing price in %s" % item)
 ```
 ### Viết item vào tệp JSON
  Pipeline sau đây lưu trữ tất cả các items đã cào (từ tất cả spiders) vào một tệp `items.jl`, chứa mỗi item trên mỗi dòng, được tuần thự theo định dạng JSON:
  ```
  import json

  class JsonWriterPipeline(object):

    def __init__(self):
        self.file = open('items.jl', 'wb')

    def process_item(self, item, spider):
        line = json.dumps(dict(item)) + "\n"
        self.file.write(line)
        return item
  ```
 ### Ghi các item vào MongoDB
  Ghi các items vào [MongoDB](https://www.mongodb.com/) bằng cách dùng [pymongo](https://api.mongodb.com/python/current/). Địa chỉ MongoDB và tên cơ sở dữ liệu được chỉ định trong phần Cài đặt scrapy.

  Điểm chính của ví dụ này là chỉ ra cách sử dụng phương thức `from_crawler()` và cách làm sạch các tài nguyên đúng cách:
  ```
  import pymongo
  
  class MongoPipeline(object):
    
    collection_name = 'scrapy_items'
    
    def __init__(self, mongo_uri, mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db
    
    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            mongo_uri=crawler.settings.get('MONGO_URI'),
            mongo_db=crawler.settings.get('MONGO_DATABASE', 'items')
        )
    
    def open_spider(self, spider):
        self.client = pymongo.MongoClient(self.mongo_uri)
        self.db = self.client[self.mongo_db]
    
    def close_spider(self, spider):
        self.client.close()
    
    def process_item(self, item, spider):
        self.db[self.collection_name].insert_one(dict(item))
        return item
  ```
